{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "re.compile('<title>(.*)</title>')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.utils import shuffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the news items\n",
    "\n",
    "charlie = pd.read_csv('charlie.csv')\n",
    "charlie = shuffle(charlie)\n",
    "ferguson = pd.read_csv('ferguson.csv')\n",
    "ferguson=shuffle(ferguson)\n",
    "german = pd.read_csv('germanwings.csv')\n",
    "german=shuffle(german)\n",
    "ottawashooting = pd.read_csv('ottawa.csv')\n",
    "ottawashooting=shuffle(ottawashooting)\n",
    "sydneysiege = pd.read_csv('sydneysiege.csv')\n",
    "sydneysiege=shuffle(sydneysiege)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exctracting labels\n",
    "\n",
    "labels = charlie['class'].map(lambda x : 1 if int(x) > 0 else 0)\n",
    "labels1 = ferguson['class'].map(lambda x : 1 if int(x) > 0 else 0)\n",
    "labels2 = german['class'].map(lambda x : 1 if int(x) > 0 else 0)\n",
    "labels3 = ottawashooting['class'].map(lambda x : 1 if int(x) > 0 else 0)\n",
    "labels4 = sydneysiege['class'].map(lambda x : 1 if int(x) > 0 else 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text by removing faaltu cheeze\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vaibhav.singh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the cleaning fx\n",
    "\n",
    "charlie['text'] = charlie['text'].map(lambda x: clean_text(x))\n",
    "ferguson['text'] = ferguson['text'].map(lambda x: clean_text(x))\n",
    "german['text'] = german['text'].map(lambda x: clean_text(x))\n",
    "ottawashooting['text'] = ottawashooting['text'].map(lambda x: clean_text(x))\n",
    "sydneysiege['text'] = sydneysiege['text'].map(lambda x: clean_text(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#appending the documents as whole\n",
    "\n",
    "documents = charlie['text']\n",
    "#print(documents.shape)\n",
    "documents = documents.append(ferguson['text'],ignore_index = True)\n",
    "#print(documents.shape)\n",
    "documents = documents.append(german['text'],ignore_index = True)\n",
    "#print(documents.shape)\n",
    "documents = documents.append(ottawashooting['text'],ignore_index = True)\n",
    "#print(documents.shape)\n",
    "documents = documents.append(sydneysiege['text'],ignore_index = True)\n",
    "#print(documents.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#appending labels.\n",
    "labels=labels.append(labels1,ignore_index=True)\n",
    "labels=labels.append(labels2,ignore_index=True)\n",
    "labels=labels.append(labels3,ignore_index=True)\n",
    "labels=labels.append(labels4,ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = charlie['text']\n",
    "lab = charlie['class'].map(lambda x : 1 if int(x) > 0 else 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145460\n",
      "WARNING:tensorflow:From /Users/vaibhav.singh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/vaibhav.singh/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 70, 100)           512600    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 70, 100)           60400     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 70, 200)           160800    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 70, 400)           641600    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 70, 800)           2563200   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 56000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 56001     \n",
      "=================================================================\n",
      "Total params: 3,994,601\n",
      "Trainable params: 3,994,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# originally only for charlie hebdo  documents = charlie['text']\n",
    "\n",
    "vocabulary_size = 50000\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "data = pad_sequences(sequences, maxlen=70)\n",
    "\n",
    "print(data.size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=70))\n",
    "model.add(Bidirectional(LSTM(50, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(400, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/vaibhav.singh/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1890 samples, validate on 188 samples\n",
      "Epoch 1/5\n",
      "1890/1890 [==============================] - 51s 27ms/step - loss: 0.5796 - acc: 0.7751 - val_loss: 0.3481 - val_acc: 0.8457\n",
      "Epoch 2/5\n",
      "1890/1890 [==============================] - 45s 24ms/step - loss: 0.2525 - acc: 0.8868 - val_loss: 0.1959 - val_acc: 0.9362\n",
      "Epoch 3/5\n",
      "1890/1890 [==============================] - 45s 24ms/step - loss: 0.0956 - acc: 0.9661 - val_loss: 0.5104 - val_acc: 0.8617\n",
      "Epoch 4/5\n",
      "1890/1890 [==============================] - 45s 24ms/step - loss: 0.0773 - acc: 0.9788 - val_loss: 0.4452 - val_acc: 0.8936\n",
      "Epoch 5/5\n",
      "1890/1890 [==============================] - 45s 24ms/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.3806 - val_acc: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2b49fc50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(np.array(data),np.array(lab), validation_split=0.09, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
